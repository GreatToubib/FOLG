function W = initializationFunction( m , choice)
% https://medium.com/@sakeshpusuluri123/activation-functions-and-weight-initialization-in-deep-learning-ebc326e62a5c

    if choice == 'zeros' % vanishing gradients problem, les poids  ne seront pas modifiés. 
        W = zeros (m,20);

    elseif choice =='random' % random avec 0.05 minimum pour éviter les vanishing gradients
       W = rand(m,20) ;
       W = 0.10+W*0.90;

    elseif choice =='xavier' % (uniform), le plus utilise avec sigmoid askip
       x = sqrt(6/(1+20)); % 1 inout et 20 output par neurone
       W = (rand(m,20) * 2 - 1) * x ;

    elseif choice =='he' % he uniform, le + utilise avec relu askip
       x = sqrt(6/1); % 1 inout
       W = (rand(m,20) * 2 - 1) * x ;

    else
        W = ones (m,20) * choice ;
    end

end

